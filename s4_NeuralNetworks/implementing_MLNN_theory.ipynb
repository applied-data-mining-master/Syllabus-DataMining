{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Artificial Neural Network \n",
    "\n",
    "Deep learning can be understood as a set of algorithms that were developed to train artificial neural networks with many layers most efficiently. In this course you will learn the basic concepts of artificial neural networks so that you will be well-equipped for, which will introduce advanced Python-based deep learning libraries and Deep Neural Network (DNN) architectures that are particularly well-suited for image and text analyses.\n",
    "\n",
    "The topics that we will cover in this chapter are as follows:\n",
    "\n",
    "  * Getting a conceptual understanding of multilayer neural networks\n",
    "  * Implementing the fundamental backpropagation algorithm for neural network training from scratch\n",
    "  * Training a basic multilayer neural network for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling complex functions\n",
    "\n",
    "Artificial neurons represent the building blocks of the multilayer artificial neural networks. The basic concept behind artificial neural networks was built upon hypotheses and models of how the human brain works to solve complex problem tasks. Although artificial neural networks have gained a lot of popularity in recent years, early studies of neural networks go back to the 1940s when Warren McCulloch and Walter Pitt first described how neurons could work. \n",
    "\n",
    "However, in the decades that followed the first implementation of the McCulloch-Pitt neuron model—Rosenblatt's perceptron in the 1950s, many researchers and machine learning practitioners slowly began to lose interest in neural networks since no one had a good solution for training a neural network with multiple layers. Eventually, interest in neural networks was rekindled in 1986 when D.E. Rumelhart, G.E. Hinton, and R.J. Williams were involved in the (re)discovery and popularization of the backpropagation algorithm to train neural networks more efficiently.\n",
    "\n",
    "> Read the Wikipedia article on AI winter, which are the periods of time where a large portion of the research community lost interest in the study of neural networks [AI_winter](https://en.wikipedia.org/wiki/AI_winter).\n",
    "\n",
    "However, neural networks have never been as popular as they are today, thanks to the many major breakthroughs that have been made in the previous decade, which resulted in what we now call deep learning algorithms and architectures—neural networks that are composed of many layers. As of today, complex neural networks powered by deep learning algorithms are considered state of the art when it comes to complex problem solving such as image and voice recognition. Popular examples of the products in our everyday life that are powered by deep learning are Google's image search and Google Translate—an application for smartphones that can automatically recognize text in images for real-time translation into more than 20 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many exciting applications of DNNs have been developed at major tech companies and the pharmaceutical industry as listed in the following, non-comprehensive list of examples:\n",
    "\n",
    "  * Facebook's DeepFace for tagging images.\n",
    "  * Baidu's DeepSpeech, which is able to handle voice queries in Mandarin.\n",
    "  * Google's new language translation service.\n",
    "  * Novel techniques for drug discovery and toxicity prediction\n",
    "  * A mobile application that can detect skin cancer with an accuracy similar to professionally trained dermatologists.\n",
    "  \n",
    "> Lectures\n",
    "> * DeepFace: Closing the Gap to Human-Level Performance in Face Verification, Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1701–1708, 2014.)\n",
    "> * DeepSpeech: Scaling up end-to-end speech recognition, A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and Andrew Y. Ng, arXiv preprint arXiv:1412.5567, 2014)\n",
    "> * Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, arXiv preprint arXiv:1412.5567, 2016)\n",
    "> * Toxicity prediction using Deep Learning, T. Unterthiner, A. Mayr, G. Klambauer, and S. Hochreiter, arXiv preprint arXiv:1503.01445, 2015.\n",
    "> Dermatologist-level classification of skin cancer with deep neural networks, A. Esteva, B.Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S.Thrun, in Nature 542, no. 7639, 2017, pages 115-118."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-layer neural network recap\n",
    "\n",
    "Bbefore we dig deeper into a particular multilayer neural network architecture, let's briefly reiterate some of the concepts of single-layer neural networks namely, the ADAptive LInear NEuron (Adaline) algorithm, which is shown in the following figure:\n",
    "\n",
    "<img src=\"img/fig1.jpeg\" width=\"500\" height=\"350\" >\n",
    "\n",
    "Previously, we implemented the Adaline algorithm to perform binary classification, and we used the gradient descent optimization algorithm to learn the weight coefficients of the model. In every epoch (pass over the training set), we updated the weight vector w using the following update rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "w := w + \\Delta{}w, \\text{ Where } \\Delta{}W = - \\eta{} \\nabla{}J \\left( \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, we computed the gradient based on the whole training set and updated the weights of the model by taking a step into the opposite direction of the gradient $\\nabla{}J \\left( w \\right)$. In order to find the optimal weights of the model, we optimized an objective function that we defined as the Sum of Squared Errors (SSE) cost function $\\nabla{}J \\left( w \\right)$. Furthermore, we multiplied the gradient by a factor, the learning rate $\\eta$ , which we had to choose carefully to balance the speed of learning against the risk of overshooting the global minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent optimization, we updated all weights simultaneously after each epoch, and we defined the partial derivative for each weight $w_j$ in the weight vector **w** as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial{}}{\\partial w_j} J \\left( w \\right) = - \\sum_i \\left( y^{(i)} - a^{(i)} \\right) x^{(i)}_j\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $y^{(i)}$ is the target class label of a particular sample $x^{(i)}$, and $a^{(i)}$  is the activation of the neuron, which is a linear function in the special case of Adaline. Furthermore, we defined the activation function $\\phi{}\\left( \\cdot \\right)$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi \\left( z \\right) = z = a\n",
    "\\end{equation*}\n",
    "\n",
    "Here, the net input **z** is a linear combination of the weights that are connecting the input to the output layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "z = \\sum_j w_j x_j = w^T x\n",
    "\\end{equation*}\n",
    "\n",
    "While we used the activation $\\phi \\left(z \\right)$  to compute the gradient update, we implemented a threshold function to squash the continuous valued output into binary class labels for prediction:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} = \\left\\{ \\begin{matrix}\n",
    "1 \\text{ if } g(z) \\geq 0 \\\\\n",
    "-1 \\text{ otherwise }\\\\\n",
    "\\end{matrix} \\right.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned about a certain trick to accelerate the model learning, the so-called stochastic gradient descent optimization. Stochastic gradient descent approximates the cost from a single training sample (online learning) or a small subset of training samples (mini-batch learning). Apart from faster learning its noisy nature is also regarded as beneficial when training multilayer neural networks with non-linear activation functions, which do not have a convex cost function. Here, the added noise can help to escape local cost minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the multilayer neural network architecture\n",
    "\n",
    "You will learn how to connect multiple single neurons to a multilayer feedforward neural network; this special type of fully connected network is also called Multilayer Perceptron (MLP). The following figure illustrates the concept of an MLP consisting of three layers:\n",
    "\n",
    "<img src=\"img/fig2.jpeg\" width=\"500\" height=\"350\" >\n",
    "\n",
    "The MLP depicted in the preceding figure has one input layer, one hidden layer, and one output layer. The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer. If such a network has more than one hidden layer, we also call it a **deep artificial neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the preceding figure, we denote the **i**-th activation unit in the **l**-th layer as $a^{(l)}_i$. To make the math and code implementations a bit more intuitive, we will not use numerical indices to refer to layers, but we will use the **in** superscript for the input layer, the **h** superscript for the hidden layer, and the **o** superscript for the output layer. For instance, $a^{(in)}_i$ refers to the ith value in the input layer, $a^{(h)}_i$ refers to the ith unit in the hidden layer, and $a^{(out)}_i$ refers to the ith unit in the output layer. Here, the activation units $a^{(in)}_0$ and $a^{(h)}_0$ are the bias units, which we set equal to 1. The activation of the units in the input layer is just its input plus the bias unit:\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{(in)} = \\begin{bmatrix}\n",
    "a^{(in)}_0 \\\\\n",
    "a^{(in)}_1 \\\\\n",
    "\\vdots \\\\\n",
    "a^{(in)}_m\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x^{(in)}_1 \\\\\n",
    "\\vdots \\\\\n",
    "x^{(in)}_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unit in layer $l$ is connected to all units in layer $l + 1$  via a weight coefficient. For example, the connection between the kth unit in layer l to the jth unit in layer $l + 1$  will be written as $w^{(l)}_{k,j}$. Referring back to the previous figure, we denote the weight matrix that connects the input to the hidden layer as $W^{(h)}$, and we write the matrix that connects the hidden layer to the output layer as $W^{(out)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While one unit in the output layer would suffice for a binary classification task, we saw a more general form of a neural network in the preceding figure, which allows us to perform multiclass classification via a generalization of the **One-versus-All (OvA)** technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As introduced earlier, we summarize the weights that connect the input and hidden layers by a matrix $W^{(h)} \\in \\mathbb{R}^{mxd}$ , where d is the number of hidden units and m is the number of input units including the bias unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize what we have just learned in a descriptive illustration of a simplified 3-4-3 multilayer perceptron:\n",
    "\n",
    "<img src=\"img/fig3.jpeg\" width=\"500\" height=\"350\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activating a neural network via forward propagation\n",
    "\n",
    "We will describe the process of forward propagation to calculate the output of an MLP model. To understand how it fits into the context of learning an MLP model, let's summarize the MLP learning procedure in three simple steps:\n",
    "\n",
    "  1. Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output.\n",
    "  2. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later.\n",
    "  3. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model.\n",
    "  \n",
    "Finally, after we repeat these three steps for multiple epochs and learn the weights of the MLP, we use forward propagation to calculate the network output and apply a threshold function to obtain the predicted class labels in the one-hot representation, which we described in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's walk through the individual steps of forward propagation to generate an output from the patterns in the training data. Since each unit in the hidden layer is connected to all units in the input layers, we first calculate the activation unit of the hidden layer $a^{(h)}_1$ as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "z^{(h)}_1 = a^{(in)}_0 w^{(in)}_{0,1} + a^{(in)}_1 w^{(h)}_{1,1} + \\dots + a^{(in)}_m w^{(h)}_{m,1}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{(h)}_1 = \\phi \\left(z^{(h)}_1 \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $z^{(h)}_1$ s the net input and $\\phi (\\cdot)$ is the activation function, which has to be differentiable to learn the weights that connect the neurons using a gradient-based approach. To be able to solve complex problems such as image classification, we need non-linear activation functions in our MLP model, for example, the sigmoid (logistic) activation function that we remember from the section about logistic regression\n",
    "\n",
    "\\begin{equation*}\n",
    "\\phi (z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP is a typical example of a feedforward artificial neural network. The term feedforward refers to the fact that each layer serves as the input to the next layer without loops, in contrast to recurrent neural networks—an architecture that we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of code efficiency and readability, we will now write the activation in a more compact form using the concepts of basic linear algebra:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "z^{(h)} = a^{(in)}W^{(h)} \\\\\n",
    "a^{(h)} = \\phi \\left( z^{(h)} \\right)\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $a^{(in)}$ is our $1 x m$ dimensional feature vector of a sample $x^{(in)}$ plus a bias unit. $W^{(h)}$ is an $m x d$ dimensional weight matrix where $d$ is the number of units in the hidden layer. After matrix-vector multiplication, we obtain the $1 x d$ dimensional net input vector $z^{(h)}$ to calculate the activation $a^{(h)}$  (where $a^{(h)} \\in \\mathbb{R}^{1xd}$). Furthermore, we can generalize this computation to all $n$ samples in the training set:\n",
    "\n",
    "\\begin{equation*}\n",
    "Z^{(h)} = A^{(in)} W^{(h)}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $A^{(in)}$ is now an $n x m$ matrix, and the matrix-matrix multiplication will result in an $n x d$ dimensional net input matrix $Z^{(h)}$. Finally, we apply the activation function $\\phi \\left( \\cdot \\right)$ to each value in the net input matrix to get the $n x d$ activation matrix $A^{(h)}$ for the next layer (here, the output layer):\n",
    "\n",
    "\\begin{equation*}\n",
    "A^{(h)} = \\phi \\left( Z^{(h)} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Similarly, we can write the activation of the output layer in vectorized form for multiple samples:\n",
    "\n",
    "\\begin{equation*}\n",
    "Z^{(out)} = A^{(h)}W^{(out)}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, we multiply the $d x t$ matrix $W^{(out)}$ ($t$ is the number of output units) by the $n x d$ dimensional matrix $A^{(h)}$ to obtain the $n x t$ dimensional matrix $Z^{(out)}$ (the columns in this matrix represent the outputs for each sample).\n",
    "\n",
    "Lastly, we apply the sigmoid activation function to obtain the continuous valued output of our network:\n",
    "\n",
    "\\begin{equation*}\n",
    "A^{(out)} = \\phi \\left( Z^{(out)} \\right), A^{(out)} \\in \\mathbb{R}^{n \\times t}\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
