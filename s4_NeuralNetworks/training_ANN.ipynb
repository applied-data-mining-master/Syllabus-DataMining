{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an artificial neural network\n",
    "\n",
    "Now, let's dig a little bit deeper into some of the concepts, such as the logistic cost function and the backpropagation algorithm.\n",
    "\n",
    "> If you are unfamiliar with calculus or need a brief refresher, consider reading this text as an additional supporting resource before continuing with this course. [Here](https://sebastianraschka.com/pdf/books/dlb/appendix_d_calculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the logistic cost function\n",
    "\n",
    "The logistic cost function that will be implemented as the `_compute_cost` method is actually pretty simple to follow since it is the same cost function that we described in the *logistic regression*\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = -\\sum^n_{i=1} y^{[i]} log \\left(a^{[i]} \\right) + \\left(1 - y^{[i]} \\right) log \\left(1 - a^{[i]} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $a^{[i]}$ is the sigmoid activation of the ith sample in the dataset, which we compute in the forward propagation step:\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{[i]} = \\phi \\left( z^{[i]} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Again, note that in this context, the superscript [i] is an index for training samples, not layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a regularization term, which allows us to reduce the degree of overfitting. L2 regularization term is defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "L2 = \\lambda \\| w \\|^2_2 = \\lambda \\sum^m_{j=1} w^2_j\n",
    "\\end{equation*}\n",
    "\n",
    "By adding the L2 regularization term to our logistic cost function, we obtain the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = - \\left[ \\sum^n_{i=1} y^{[i]} log \\left(a^{[i]} \\right) + \\left(1 - y^{[i]} \\right) log \\left(1 - a^{[i]} \\right) \\right] + \\frac{\\lambda}{2} \\| w \\|^2_2\n",
    "\\end{equation*}\n",
    "\n",
    "Since we will  implement an MLP for multiclass classification that returns an output vector of t elements that we need to compare to the $t \\times 1$ dimensional target vector in the one-hot encoding representation, for example, the activation of the third layer and the target class (here, class 2) for a particular sample may look like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "a^{(out)} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.9 \\\\\n",
    "\\vdots \\\\\n",
    "0.3\n",
    "\\end{bmatrix}, y = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, we need to generalize the logistic cost function to all t activation units in our network. Thus, the cost function (without the regularization term) becomes the following:\n",
    "\n",
    "\\begin{equation*}\n",
    " \\left( w \\right) = - \\sum^n_{i=1} \\sum^t_{j=1} y^{[i]}_j log \\left(a^{[i]}_j \\right) + \\left(1 - y^{[i]}_j \\right) log \\left(1 - a^{[i]}_j \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Here, again, the superscript **(i)** is the index of a particular sample in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generalized regularization term may look a little bit complicated at first, but here we are just calculating the sum of all weights of an $l$ layer (without the bias term) that we added to the first column:\n",
    "\n",
    "\\begin{equation*}\n",
    "J \\left( w \\right) = - \\left[ \\sum^n_{i=1} \\sum^t_{j=1} y^{[i]}_j log \\left(a^{[i]}_j \\right) + \\left(1 - y^{[i]}_j \\right) log \\left(1 - a^{[i]}_j \\right) \\right] + \\frac{\\lambda}{2} \\sum^{L-1}_{l=1} \\sum^{u_l}_{i=1} \\sum^{u_{l+1}}_{j=1} \\left( w^{(l)}_{j, i} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $u_1$ refers to the number of units in a given layer $l$, and the following expression represents the penalty term:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\lambda}{2} \\sum^{L-1}_{l=1} \\sum^{u_l}_{i=1} \\sum^{u_{l+1}}_{j=1} \\left( w^{(l)}_{j, i} \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "Remember that our goal is to minimize the cost function $J \\left(W \\right)$; thus we need to calculate the partial derivative of the parameters $W$ with respect to each weight for every layer in the network:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial}{\\partial w^{(l)}_{j, i}} J \\left( W \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $W$ consists of multiple matrices. In a multilayer perceptron with one hidden unit, we have the weight matrix $W^{(h)}$, which connects the input to the hidden layer, and $W^{(out)}$, which connects the hidden layer to the output layer. An intuitive visualization of the three-dimensional tensor $W$ is provided in the following figure:\n",
    "\n",
    "<img src=\"img/fig4.jpg\" width=\"400\" height=\"300\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simplified figure, it may seem that both $W^{(h)}$ and $W^{(out)}$ have the same number of rows and columns, which is typically not the case unless we initialize an MLP with the same number of hidden units, output units, and input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation\n",
    "\n",
    "Although backpropagation was rediscovered and popularized more than 30 years ago, it still remains one of the most widely used algorithms to train artificial neural networks very efficiently. \n",
    "\n",
    "  > Readings\n",
    "  > * Learning representations by back-propagating errors, D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Nature, 323: 6088, pages 533â€“536, 1986\n",
    "  > * [Who Invented Backpropagation?](http://people.idsia.ch/~juergen/who-invented-backpropagation.html)\n",
    "  \n",
    "In essence, we can think of backpropagation as a very computationally efficient approach to compute the partial derivatives of a complex cost function in multilayer neural networks. Here, our goal is to use those derivatives to learn the weight coefficients for parameterizing such a multilayer artificial neural network. The challenge in the parameterization of neural networks is that we are typically dealing with a very large number of weight coefficients in a high-dimensional feature space. In contrast to cost functions of single-layer neural networks such as Adaline or logistic regression. the error surface of a neural network cost function is not convex or smooth with respect to the parameters. There are many bumps in this high-dimensional cost surface (local minima) that we have to overcome in order to find the global minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous section, we saw how to calculate the cost as the difference between the activation of the last layer and the target class label. Now, we will see how the backpropagation algorithm works to update the weights in our MLP model from a mathematical perspective, which it must be  implemented in the `Backpropagation` section inside the `fit` method.\n",
    "\n",
    "First need to apply forward propagation in order to obtain the activation of the output layer, which we formulated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "Z^{(h)} = A^{(in)}W^{(h)} & \\text{net input of the hidden layer} \\\\\n",
    "A^{(h)} = \\phi \\left( Z^{(h)} \\right) & \\text{activation of the hidden layer} \\\\\n",
    "Z^{(out)} = A^{(h)}W^{(out)} & \\text{net input of the output layer} \\\\\n",
    "A^{(out)} = \\phi \\left(Z^{(out)} \\right) & \\text{activation of the output layer}\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Concisely, we just forward-propagate the input features through the connection in the network, as shown in the following illustration:\n",
    "\n",
    "<img src=\"img/fig5.jpg\" width=\"400\" height=\"300\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backpropagation, we propagate the error from right to left. We start by calculating the error vector of the output layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta^{(out)} = a^{(out)} - y\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $y$ is the vector of the true class labels.\n",
    "\n",
    "Next, we calculate the error term of the hidden layer:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta^{(h)} = \\delta^{(out)} \\left( W^{(out)} \\right)^T \\odot \\frac{\\partial \\phi \\left( z^{(h)} \\right)}{\\partial z^{(h)}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, $\\frac{\\partial \\phi \\left( z^{(h)} \\right)}{\\partial z^{(h)}}$ is simply the derivative of the sigmoid activation function.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial \\phi \\left(z \\right)}{\\partial z} = \\left( a^{(h)} \\odot \\left( 1 -a^{(h)} \\right) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the $\\odot$  symbol means element-wise multiplication in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the $\\delta^{(h)}$ layer error matrix (`sigma_h`) as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta^{(h)} = \\delta^{(out)} \\left( W^{(out)} \\right)^T \\odot \\left( a^{(h)} \\odot \\left( 1 - a^{(h)} \\right) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "To better understand how we computed this $\\delta^{(h)}$ term, let's walk through it in more detail. In the preceding equation, we used the transpose $\\left( W^{(out)} \\right)^T$ of the $h \\times t$-dimensional matrix $W^{(out)}$. Here, $t$ is the number of output class labels and $h$ is the number of hidden units. The matrix multiplication between the $n \\times t$-dimensional $\\delta^{(out)}$ matrix and the $t \\times h$-dimensional matrix $ \\left( W^{(out)} \\right)^T$, results in an $n \\times t$-dimensional matrix that we multiplied elementwise by the sigmoid derivative of the same dimension to obtain the $n \\times t$-dimensional matrix $\\delta^{(h)}$.\n",
    "\n",
    "Eventually, after obtaining the $\\delta$ terms, we can now write the derivation of the cost function as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial}{\\partial w^{(out)}_{i, j}} J \\left( W \\right) = a^{(h)}_j \\delta^{(out)}_i \\\\\n",
    "\\frac{\\partial}{\\partial w^{(h)}_{i, j}} J \\left( W \\right) = a^{(in)}_j \\delta^{(h)}_i\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Next, we need to accumulate the partial derivative of every node in each layer and the error of the node in the next layer. However, remember that we need to compute $\\Delta^{(l)}_{i, j}$ for every sample in the training set. Thus, it is easier to implement it as a vectorized version:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{matrix}\n",
    "\\Delta^{(h)} = \\Delta^{(h)} + \\left( A^{(in)} \\right)^T \\delta^{(h)} \\\\\n",
    "\\Delta^{(out)} = \\Delta^{(out)} + \\left( A^{(h)} \\right)^T \\delta^{(out)}\n",
    "\\end{matrix}\n",
    "\\end{equation*}\n",
    "\n",
    "And after we have accumulated the partial derivatives, we can add the regularization term:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta^{(l)} := \\Delta^{(l)} + \\lambda^{(l)} \\text{( except for the bias term)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, after we have computed the gradients, we can now update the weights by taking an opposite step towards the gradient for each layer $l$:\n",
    "\n",
    "\\begin{equation*}\n",
    "W^{(l)} := W^{(l)} - \\eta \\Delta^{(l)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring everything together, let's summarize backpropagation in the following figure:\n",
    "\n",
    "<img src=\"img/fig6.jpg\" width=\"400\" height=\"300\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
